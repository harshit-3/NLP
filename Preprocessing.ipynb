{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d8fd547-6f95-44e3-abbd-14ca275fdeb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc3ae53e-0844-43ea-848b-ec7d71c079af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>premise</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pluto rotates once on its axis every 6.39 Eart...</td>\n",
       "      <td>Earth rotates on its axis once times in one day.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>---Glenn =====================================...</td>\n",
       "      <td>Earth rotates on its axis once times in one day.</td>\n",
       "      <td>entails</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>geysers - periodic gush of hot water at the su...</td>\n",
       "      <td>The surface of the sun is much hotter than alm...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Facts: Liquid water droplets can be changed in...</td>\n",
       "      <td>Evaporation is responsible for changing liquid...</td>\n",
       "      <td>entails</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>By comparison, the earth rotates on its axis o...</td>\n",
       "      <td>Earth rotates on its axis once times in one day.</td>\n",
       "      <td>entails</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             premise  \\\n",
       "0  Pluto rotates once on its axis every 6.39 Eart...   \n",
       "1  ---Glenn =====================================...   \n",
       "2  geysers - periodic gush of hot water at the su...   \n",
       "3  Facts: Liquid water droplets can be changed in...   \n",
       "4  By comparison, the earth rotates on its axis o...   \n",
       "\n",
       "                                          hypothesis    label  \n",
       "0   Earth rotates on its axis once times in one day.  neutral  \n",
       "1   Earth rotates on its axis once times in one day.  entails  \n",
       "2  The surface of the sun is much hotter than alm...  neutral  \n",
       "3  Evaporation is responsible for changing liquid...  entails  \n",
       "4   Earth rotates on its axis once times in one day.  entails  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_json(\"train.json\")\n",
    "test = pd.read_json(\"test.json\")\n",
    "validation = pd.read_json(\"validation.json\")\n",
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35747c67-da04-449d-bb70-838deba80c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    text = text.lower()  # lowercase\n",
    "    \n",
    "    # replace non-digit dots with space\n",
    "    text = re.sub(r'(?<!\\d)\\.(?!\\d)', ' ', text)\n",
    "    \n",
    "    # remove everything except letters, numbers, dot-in-numbers, spaces\n",
    "    text = re.sub(r'[^a-z0-9.\\s]', ' ', text)\n",
    "    \n",
    "    # normalize whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1b3c59-650c-40be-a848-be5f79becde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply clean text on train\n",
    "train[\"premise_clean\"] = train[\"premise\"].apply(clean_text)\n",
    "train[\"hypothesis_clean\"]  = train[\"hypothesis\"].apply(clean_text)\n",
    "\n",
    "\n",
    "# apply clean text on test\n",
    "test[\"premise_clean\"] = test[\"premise\"].apply(clean_text)\n",
    "test[\"hypothesis_clean\"]  = test[\"hypothesis\"].apply(clean_text)\n",
    "\n",
    "\n",
    "# apply clean text on validation\n",
    "validation[\"premise_clean\"] = validation[\"premise\"].apply(clean_text)\n",
    "validation[\"hypothesis_clean\"]  = validation[\"hypothesis\"].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5dd615",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to C:\\Users\\lucac.DESKTOP-\n",
      "[nltk_data]     V7T4SBD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import required libraries\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "from nltk.tokenize import word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0c4521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define contractions dictionary\n",
    "contraction_dict = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\",\n",
    "                    \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\",\n",
    "                    \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\",\n",
    "                    \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\",\n",
    "                    \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\",\n",
    "                    \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
    "                    \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\",\n",
    "                    \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\",\n",
    "                    \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\n",
    "                    \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\",\n",
    "                    \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\",\n",
    "                    \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\n",
    "                    \"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
    "                    \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
    "                    \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\",\n",
    "                    \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\",\n",
    "                    \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\",\n",
    "                    \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\",\n",
    "                    \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\",\n",
    "                    \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\",\n",
    "                    \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "                    \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bf86d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing premises...\n",
      "Tokenizing hypotheses...\n",
      "Example tokenized premise: ['pluto', 'rotates', 'once', 'on', 'its', 'axis', 'every', '639', 'earth', 'days']\n",
      "Example tokenized hypothesis: ['earth', 'rotates', 'on', 'its', 'axis', 'once', 'times', 'in', 'one', 'day']\n"
     ]
    }
   ],
   "source": [
    "# define preprocessing function\n",
    "def pre_process(sent_list):\n",
    "    \"\"\"\n",
    "    Preprocess a list of sentences:\n",
    "    - Handle contractions\n",
    "    - Remove punctuation\n",
    "    - Tokenize\n",
    "    \"\"\"\n",
    "    output = []\n",
    "    for sent in sent_list:\n",
    "        sent = sent.lower() # case-folding (already done, but keeping for consistency)\n",
    "        for word, new_word in contraction_dict.items():\n",
    "            sent = sent.replace(word, new_word) # dealing with contractions\n",
    "        sent = re.sub(r'[^\\w\\s]','',sent) # removing punctuation\n",
    "        output.append(word_tokenize(sent)) # tokenization\n",
    "    return output\n",
    "\n",
    "# Apply preprocessing to get tokenized lists\n",
    "print(\"Tokenizing premises...\")\n",
    "train_premise_tokens = pre_process(train[\"premise_clean\"].tolist())\n",
    "test_premise_tokens = pre_process(test[\"premise_clean\"].tolist())\n",
    "val_premise_tokens = pre_process(validation[\"premise_clean\"].tolist())\n",
    "\n",
    "print(\"Tokenizing hypotheses...\")\n",
    "train_hypothesis_tokens = pre_process(train[\"hypothesis_clean\"].tolist())\n",
    "test_hypothesis_tokens = pre_process(test[\"hypothesis_clean\"].tolist())\n",
    "val_hypothesis_tokens = pre_process(validation[\"hypothesis_clean\"].tolist())\n",
    "\n",
    "print(f\"Example tokenized premise: {train_premise_tokens[0]}\")\n",
    "print(f\"Example tokenized hypothesis: {train_hypothesis_tokens[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470e70eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 20499\n",
      "First 20 words in vocab: ['<PAD>', '<UNK>', 'pluto', 'rotates', 'once', 'on', 'its', 'axis', 'every', '639', 'earth', 'days', 'glenn', 'per', 'day', 'the', 'about', 'geysers', 'periodic', 'gush']\n"
     ]
    }
   ],
   "source": [
    "word_to_ix = {\"<PAD>\": 0, \"<UNK>\": 1}  # PAD for padding, UNK for unknown words\n",
    "\n",
    "# build vocabulary from training data only\n",
    "for sentence in train_premise_tokens + train_hypothesis_tokens:\n",
    "    for word in sentence:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "\n",
    "word_list = list(word_to_ix.keys())\n",
    "vocab_size = len(word_to_ix)\n",
    "\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "print(f\"First 20 words in vocab: {word_list[:20]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328c1529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example indexed premise: [2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "Example indexed hypothesis: [10, 3, 5, 6, 7, 4, 48, 78, 60, 14]\n"
     ]
    }
   ],
   "source": [
    "# convert tokens to indices\n",
    "def to_index(data, to_ix):\n",
    "    \"\"\"\n",
    "    Convert token lists to index lists\n",
    "    Use <UNK> token for words not in vocabulary\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for sentence in data:\n",
    "        indexed_sentence = []\n",
    "        for word in sentence:\n",
    "            if word in to_ix:\n",
    "                indexed_sentence.append(to_ix[word])\n",
    "            else:\n",
    "                indexed_sentence.append(to_ix[\"<UNK>\"])  # Unknown word\n",
    "        result.append(indexed_sentence)\n",
    "    return result\n",
    "\n",
    "# convert all datasets to indices\n",
    "train_premise_idx = to_index(train_premise_tokens, word_to_ix)\n",
    "train_hypothesis_idx = to_index(train_hypothesis_tokens, word_to_ix)\n",
    "\n",
    "test_premise_idx = to_index(test_premise_tokens, word_to_ix)\n",
    "test_hypothesis_idx = to_index(test_hypothesis_tokens, word_to_ix)\n",
    "\n",
    "val_premise_idx = to_index(val_premise_tokens, word_to_ix)\n",
    "val_hypothesis_idx = to_index(val_hypothesis_tokens, word_to_ix)\n",
    "\n",
    "print(f\"Example indexed premise: {train_premise_idx[0]}\")\n",
    "print(f\"Example indexed hypothesis: {train_hypothesis_idx[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "064ac144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max premise length: 11640\n",
      "Max hypothesis length: 36\n"
     ]
    }
   ],
   "source": [
    "# maximum length from the training set\n",
    "MAX_LENGTH_PREMISE = max([len(s) for s in train_premise_idx])\n",
    "MAX_LENGTH_HYPOTHESIS = max([len(s) for s in train_hypothesis_idx])\n",
    "\n",
    "print(f\"Max premise length: {MAX_LENGTH_PREMISE}\")\n",
    "print(f\"Max hypothesis length: {MAX_LENGTH_HYPOTHESIS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556ff362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label mapping: {'entails': 0, 'neutral': 1}\n",
      "Example labels: [1, 0, 1, 0, 0]\n",
      "Original labels: ['neutral', 'entails', 'neutral', 'entails', 'entails']\n"
     ]
    }
   ],
   "source": [
    "# encode labels\n",
    "label_to_ix = {\"entails\": 0, \"neutral\": 1}\n",
    "ix_to_label = {0: \"entails\", 1: \"neutral\"}\n",
    "\n",
    "# convert labels to indices\n",
    "train_labels = [label_to_ix[label] for label in train[\"label\"]]\n",
    "test_labels = [label_to_ix[label] for label in test[\"label\"]]\n",
    "val_labels = [label_to_ix[label] for label in validation[\"label\"]]\n",
    "\n",
    "print(f\"Label mapping: {label_to_ix}\")\n",
    "print(f\"Example labels: {train_labels[:5]}\")\n",
    "print(f\"Original labels: {train['label'].iloc[:5].tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "97cc20c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training examples: 23088\n",
      "Validation examples: 1304\n",
      "Test examples: 2126\n",
      "Vocabulary size: 20499\n",
      "Number of labels: 2\n",
      "Max premise length: 11640\n",
      "Max hypothesis length: 36\n"
     ]
    }
   ],
   "source": [
    "# final statistics\n",
    "print(f\"Training examples: {len(train_premise_idx)}\")\n",
    "print(f\"Validation examples: {len(val_premise_idx)}\")\n",
    "print(f\"Test examples: {len(test_premise_idx)}\")\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "print(f\"Number of labels: {len(label_to_ix)}\")\n",
    "print(f\"Max premise length: {MAX_LENGTH_PREMISE}\")\n",
    "print(f\"Max hypothesis length: {MAX_LENGTH_HYPOTHESIS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb278919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed data saved to 'preprocessed_data.pkl'\n"
     ]
    }
   ],
   "source": [
    "# save preprocessed data\n",
    "import pickle\n",
    "\n",
    "preprocessed_data = {\n",
    "    'train_premise_idx': train_premise_idx,\n",
    "    'train_hypothesis_idx': train_hypothesis_idx,\n",
    "    'train_labels': train_labels,\n",
    "    'test_premise_idx': test_premise_idx,\n",
    "    'test_hypothesis_idx': test_hypothesis_idx,\n",
    "    'test_labels': test_labels,\n",
    "    'val_premise_idx': val_premise_idx,\n",
    "    'val_hypothesis_idx': val_hypothesis_idx,\n",
    "    'val_labels': val_labels,\n",
    "    'word_to_ix': word_to_ix,\n",
    "    'label_to_ix': label_to_ix,\n",
    "    'ix_to_label': ix_to_label,\n",
    "    'vocab_size': vocab_size,\n",
    "    'MAX_LENGTH_PREMISE': MAX_LENGTH_PREMISE,\n",
    "    'MAX_LENGTH_HYPOTHESIS': MAX_LENGTH_HYPOTHESIS\n",
    "}\n",
    "\n",
    "with open('preprocessed_data.pkl', 'wb') as f:\n",
    "    pickle.dump(preprocessed_data, f)\n",
    "    \n",
    "print(\"Preprocessed data saved to 'preprocessed_data.pkl'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2003f1b",
   "metadata": {},
   "source": [
    "## How to Load Preprocessed Data\n",
    "\n",
    "In the model training notebook, you can load this data with:\n",
    "\n",
    "```python\n",
    "import pickle\n",
    "\n",
    "with open('preprocessed_data.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "# Access the data\n",
    "train_premise_idx = data['train_premise_idx']\n",
    "train_hypothesis_idx = data['train_hypothesis_idx']\n",
    "train_labels = data['train_labels']\n",
    "word_to_ix = data['word_to_ix']\n",
    "vocab_size = data['vocab_size']\n",
    "# ... etc\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
