{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "edc9772a-6460-45d8-8a28-ce506fdde0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50d83b4a-c59c-4d0e-96c4-454de1573004",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('preprocessed_data.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "224e244f-02b8-405a-a10f-3f4b4763fbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_premise_idx = data['train_premise_idx']\n",
    "train_hypothesis_idx = data['train_hypothesis_idx']\n",
    "train_labels = data['train_labels']\n",
    "\n",
    "val_premise_idx = data['val_premise_idx']\n",
    "val_hypothesis_idx = data['val_hypothesis_idx']\n",
    "val_labels = data['val_labels']\n",
    "\n",
    "test_premise_idx = data['test_premise_idx']\n",
    "test_hypothesis_idx = data['test_hypothesis_idx']\n",
    "test_labels = data['test_labels']\n",
    "\n",
    "vocab_size = data['vocab_size']\n",
    "word_to_ix = data['word_to_ix']\n",
    "label_to_ix = data['label_to_ix']\n",
    "MAX_LENGTH_PREMISE = min(data['MAX_LENGTH_PREMISE'], 400)  # cap long premises\n",
    "MAX_LENGTH_HYPOTHESIS = min(data['MAX_LENGTH_HYPOTHESIS'], 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "868e125a-7717-4b90-8018-a50a5268fe7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Premise lengths - Max: 11640, Mean: 18.3, 95th percentile: 33.0\n",
      "Hypothesis lengths - Max: 36, Mean: 11.8, 95th percentile: 20.0\n",
      "\n",
      "Note: Sequences will be truncated to 200 tokens to manage memory\n"
     ]
    }
   ],
   "source": [
    "# check sequence length statistics\n",
    "premise_lens = [len(p) for p in train_premise_idx]\n",
    "hyp_lens = [len(h) for h in train_hypothesis_idx]\n",
    "\n",
    "print(f\"Premise lengths - Max: {max(premise_lens)}, Mean: {np.mean(premise_lens):.1f}, 95th percentile: {np.percentile(premise_lens, 95):.1f}\")\n",
    "print(f\"Hypothesis lengths - Max: {max(hyp_lens)}, Mean: {np.mean(hyp_lens):.1f}, 95th percentile: {np.percentile(hyp_lens, 95):.1f}\")\n",
    "print(f\"\\nNote: Sequences will be truncated to {200} tokens to manage memory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "50b6fac8-0f7d-440a-898e-2097c2e8e9c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\" Using device: {device}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2b189aae-38e7-4f09-88df-40ed6222d80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset class\n",
    "class NLIDataset(Dataset):\n",
    "    def __init__(self, premise_idx, hypothesis_idx, labels):\n",
    "        self.premise_idx = premise_idx\n",
    "        self.hypothesis_idx = hypothesis_idx\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'premise': self.premise_idx[idx],\n",
    "            'hypothesis': self.hypothesis_idx[idx],\n",
    "            'label': self.labels[idx]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "26c61ee2-83ba-48b2-ac68-b88706b4acad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- replace your collate_fn with this (uses the caps you computed earlier) --\n",
    "\n",
    "MAX_SEQ_LENGTH = 400 \n",
    "PAD_ID = 0\n",
    "\n",
    "def collate_fn(batch):\n",
    "    premises = [item['premise'][:MAX_LENGTH_PREMISE] for item in batch]\n",
    "    hypotheses = [item['hypothesis'][:MAX_LENGTH_HYPOTHESIS] for item in batch]\n",
    "    labels = [item['label'] for item in batch]\n",
    "\n",
    "    max_premise_len = max(len(p) for p in premises)\n",
    "    max_hypothesis_len = max(len(h) for h in hypotheses)\n",
    "\n",
    "    def pad_to(seqs, L):\n",
    "        return [seq + [PAD_ID]*(L - len(seq)) for seq in seqs]\n",
    "\n",
    "    premises_tensor   = torch.LongTensor(pad_to(premises,   max_premise_len))\n",
    "    hypotheses_tensor = torch.LongTensor(pad_to(hypotheses, max_hypothesis_len))\n",
    "    labels_tensor     = torch.LongTensor(labels)\n",
    "\n",
    "    return premises_tensor, hypotheses_tensor, labels_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4c937eda-f071-47ee-820f-427b7c4b4293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters:\n",
      "  Batch size: 64\n",
      "  Embedding dim: 128\n",
      "  Hidden dim: 256\n",
      "  Learning rate: 0.001\n",
      "  Number of epochs: 10\n",
      "\n",
      "Dataloaders created:\n",
      "  Training batches: 361\n",
      "  Validation batches: 21\n",
      "  Test batches: 34\n"
     ]
    }
   ],
   "source": [
    "# create datasets\n",
    "train_dataset = NLIDataset(train_premise_idx, train_hypothesis_idx, train_labels)\n",
    "val_dataset = NLIDataset(val_premise_idx, val_hypothesis_idx, val_labels)\n",
    "test_dataset = NLIDataset(test_premise_idx, test_hypothesis_idx, test_labels)\n",
    "\n",
    "# hyperparameters - automatically adjust batch size based on GPU\n",
    "BATCH_SIZE = 64 if torch.cuda.is_available() else 32  # use 64 for GPU, 32 for CPU\n",
    "EMBEDDING_DIM = 128\n",
    "HIDDEN_DIM = 256\n",
    "NUM_CLASSES = 2\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "print(f\"Hyperparameters:\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Embedding dim: {EMBEDDING_DIM}\")\n",
    "print(f\"  Hidden dim: {HIDDEN_DIM}\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  Number of epochs: {NUM_EPOCHS}\")\n",
    "\n",
    "# create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "print(f\"\\nDataloaders created:\")\n",
    "print(f\"  Training batches: {len(train_loader)}\")\n",
    "print(f\"  Validation batches: {len(val_loader)}\")\n",
    "print(f\"  Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d6621d89-1f7b-480a-bb37-7d35fa3ce7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionPool(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.W = nn.Linear(dim, dim)\n",
    "        self.v = nn.Linear(dim, 1, bias=False)\n",
    "\n",
    "    def forward(self, H, mask):  # H:[B,T,D], mask:[B,T] (1=real, 0=pad)\n",
    "        scores = self.v(torch.tanh(self.W(H))).squeeze(-1)     # [B,T]\n",
    "\n",
    "        # Mask pads to large negative so they contribute ~0 after softmax\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        # If a whole row is PADs, set that row's scores to [0, -inf, -inf, ...]\n",
    "        # so softmax yields a safe one-hot at position 0 â€” without touching alpha later.\n",
    "        all_pad = (mask.sum(dim=1) == 0)                       # [B]\n",
    "        if all_pad.any():\n",
    "            # first set all to -inf\n",
    "            scores[all_pad] = -1e9\n",
    "            # then make index 0 = 0 so softmax -> 1 at idx 0\n",
    "            scores[all_pad, 0] = 0.0\n",
    "\n",
    "        alpha = torch.softmax(scores, dim=-1)                  # [B,T]\n",
    "        p_star = torch.bmm(alpha.unsqueeze(1), H).squeeze(1)   # [B,D]\n",
    "        return p_star, alpha\n",
    "\n",
    "        \n",
    "\n",
    "class BahdanauAttention(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.W_h = nn.Linear(dim, dim, bias=False)\n",
    "        self.W_s = nn.Linear(dim, dim, bias=False)\n",
    "        self.v   = nn.Linear(dim, 1,  bias=False)\n",
    "\n",
    "    def forward(self, Hk, q, mask):      # Hk:[B,Tp,2H], q:[B,2H], mask:[B,Tp] (1=real,0=pad)\n",
    "        scores = self.v(torch.tanh(self.W_h(Hk) + self.W_s(q).unsqueeze(1))).squeeze(-1)  # [B,Tp]\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        # if a row is all pads -> set [0, -inf, -inf, ...] so softmax is well-defined\n",
    "        all_pad = (mask.sum(dim=1) == 0)          # [B]\n",
    "        if all_pad.any():\n",
    "            scores[all_pad] = -1e9\n",
    "            scores[all_pad, 0] = 0.0\n",
    "\n",
    "        alpha = torch.softmax(scores, dim=-1)     # [B,Tp]\n",
    "        ctx   = torch.bmm(alpha.unsqueeze(1), Hk).squeeze(1)   # [B,2H]\n",
    "        return ctx, alpha\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d5f1f7e6-4f4b-4271-8ada-362b908091b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderSideAttentionNLI(nn.Module):\n",
    "    \"\"\"\n",
    "    Premise: BiLSTM -> SelfAttentionPool -> p*\n",
    "    Hypothesis: BiLSTM -> MaxPool -> h*\n",
    "    Classify on [p*, h*, |p*-h*|, p*âˆ˜h*]\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, emb_dim, hid_dim, num_classes, pad_idx=0, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=pad_idx)\n",
    "        self.enc_p = nn.LSTM(emb_dim, hid_dim, batch_first=True, bidirectional=True)\n",
    "        self.enc_h = nn.LSTM(emb_dim, hid_dim, batch_first=True, bidirectional=True)\n",
    "        self.pool_p = SelfAttentionPool(2*hid_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        feat_dim = 4 * (2*hid_dim)  # concat p*, h*, |.|, âˆ˜\n",
    "        self.fc1 = nn.Linear(feat_dim, 2*hid_dim)\n",
    "        self.fc2 = nn.Linear(2*hid_dim, num_classes)\n",
    "\n",
    "    def _mask(self, x, pad=0):\n",
    "        return (x != pad).long()\n",
    "\n",
    "    def _encode_bilstm(self, x, lstm):\n",
    "        emb = self.embedding(x)                      # [B,T,E]\n",
    "        H, _ = lstm(emb)                             # [B,T,2H]\n",
    "        return H\n",
    "\n",
    "    def _max_pool_time(self, H, mask):         # inside your EncoderSideAttentionNLI\n",
    "        masked = H.masked_fill(mask.unsqueeze(-1) == 0, float('-inf'))\n",
    "        pooled = masked.max(dim=1).values      # [B,D]\n",
    "        # handle all-pad rows: replace -inf with 0\n",
    "        is_all_pad = (mask.sum(dim=1) == 0)\n",
    "        if is_all_pad.any():\n",
    "            pooled[is_all_pad] = 0.0\n",
    "        return pooled\n",
    "\n",
    "    def forward(self, prem_ids, hyp_ids):\n",
    "        prem_mask = self._mask(prem_ids)             # [B,Tp]\n",
    "        hyp_mask  = self._mask(hyp_ids)              # [B,Th]\n",
    "\n",
    "        H_p = self._encode_bilstm(prem_ids, self.enc_p)       # [B,Tp,2H]\n",
    "        p_star, attn_p = self.pool_p(H_p, prem_mask)          # [B,2H], [B,Tp]\n",
    "\n",
    "        H_h = self._encode_bilstm(hyp_ids,  self.enc_h)       # [B,Th,2H]\n",
    "        h_star = self._max_pool_time(H_h, hyp_mask)           # [B,2H]\n",
    "\n",
    "        # NLI matching features\n",
    "        diff = torch.abs(p_star - h_star)\n",
    "        prod = p_star * h_star\n",
    "        z = torch.cat([p_star, h_star, diff, prod], dim=-1)   # [B,8H]\n",
    "\n",
    "        out = self.dropout(torch.relu(self.fc1(z)))\n",
    "        logits = self.fc2(out)                                 # [B,2]\n",
    "        return logits, attn_p  # return attn over premise for qualitative plots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0b728537-09d8-4e81-9422-5b7ff37ad076",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderSideAttentionNLI(nn.Module):\n",
    "    \"\"\"\n",
    "    Premise encoder: BiLSTM -> H_p (keys/values for attention)\n",
    "    Decoder over hypothesis (uni-LSTM):\n",
    "        at each step t, attend over H_p with Bahdanau attention using s_t\n",
    "    Pool over time: mean([s_t; c_t]) -> classifier\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, emb_dim, hid_dim, num_classes, pad_idx=0, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=pad_idx)\n",
    "        self.enc_p = nn.LSTM(emb_dim, hid_dim, batch_first=True, bidirectional=True)\n",
    "        self.dec_h = nn.LSTM(emb_dim + 2*hid_dim, hid_dim, batch_first=True, bidirectional=False)\n",
    "        self.attn  = BahdanauAttention(2*hid_dim)  # query dim will be hid_dim; project it to 2H first\n",
    "\n",
    "        # project decoder state to 2H to match attention query dim\n",
    "        self.proj_q = nn.Linear(hid_dim, 2*hid_dim, bias=False)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc1 = nn.Linear(hid_dim + 2*hid_dim, 2*hid_dim)  # features after pooling\n",
    "        self.fc2 = nn.Linear(2*hid_dim, num_classes)\n",
    "\n",
    "    def _mask(self, x, pad=0):\n",
    "        return (x != pad).long()\n",
    "\n",
    "    def forward(self, prem_ids, hyp_ids):\n",
    "        B = prem_ids.size(0)\n",
    "\n",
    "        prem_mask = self._mask(prem_ids)                      # [B,Tp]\n",
    "        hyp_mask  = self._mask(hyp_ids)                       # [B,Th]\n",
    "\n",
    "        # Encode premise\n",
    "        H_p, _ = self.enc_p(self.embedding(prem_ids))         # [B,Tp,2H]\n",
    "\n",
    "        # Decoder over hypothesis with attention at every step\n",
    "        # We'll run a standard LSTM but feed previous context\n",
    "        Th = hyp_ids.size(1)\n",
    "        emb_h = self.embedding(hyp_ids)                       # [B,Th,E]\n",
    "\n",
    "        # init s_0, c_0 as zeros\n",
    "        h_t = torch.zeros(1, B, self.dec_h.hidden_size, device=emb_h.device)\n",
    "        c_t = torch.zeros(1, B, self.dec_h.hidden_size, device=emb_h.device)\n",
    "        ctx_prev = torch.zeros(B, 2*self.enc_p.hidden_size, device=emb_h.device)  # [B,2H]\n",
    "\n",
    "        dec_outputs = []\n",
    "        attn_all = []  # collect attention per step for qualitative heatmap\n",
    "\n",
    "        for t in range(Th):\n",
    "            # input to decoder: [y_t ; ctx_{t-1}]\n",
    "            dec_in_t = torch.cat([emb_h[:, t, :], ctx_prev], dim=-1).unsqueeze(1)  # [B,1,E+2H]\n",
    "            dec_out_t, (h_t, c_t) = self.dec_h(dec_in_t, (h_t, c_t))               # dec_out_t: [B,1,H]\n",
    "            s_t = dec_out_t.squeeze(1)                                             # [B,H]\n",
    "\n",
    "            # attention: query is projected s_t -> 2H\n",
    "            q_t = self.proj_q(s_t)                                                 # [B,2H]\n",
    "            ctx_t, alpha_t = self.attn(H_p, q_t, prem_mask)                        # [B,2H], [B,Tp]\n",
    "\n",
    "            # store and roll\n",
    "            dec_outputs.append(torch.cat([s_t, ctx_t], dim=-1))                    # [B,H+2H]\n",
    "            attn_all.append(alpha_t.unsqueeze(1))                                  # [B,1,Tp]\n",
    "            ctx_prev = ctx_t\n",
    "\n",
    "        # pool over time (mean)\n",
    "        dec_stack = torch.stack(dec_outputs, dim=1)   # [B,Th,H+2H]\n",
    "        g = dec_stack.mean(dim=1)\n",
    "                                                 # [B,H+2H]\n",
    "\n",
    "        out = self.dropout(torch.relu(self.fc1(g)))\n",
    "        logits = self.fc2(out)                                                      # [B,2]\n",
    "\n",
    "        # concat attention maps along time: [B,Th,Tp]\n",
    "        attn_map = torch.cat(attn_all, dim=1) if attn_all else None\n",
    "        return logits, attn_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b4ffacbe-dc40-4720-a141-a0286f54273c",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss, all_preds, all_labels = 0.0, [], []\n",
    "    for prem, hyp, labels in dataloader:\n",
    "        prem, hyp, labels = prem.to(device), hyp.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits, _ = model(prem, hyp)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        all_preds.extend(logits.argmax(dim=1).detach().cpu().numpy())\n",
    "        all_labels.extend(labels.detach().cpu().numpy())\n",
    "    return total_loss/len(dataloader), accuracy_score(all_labels, all_preds)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss, all_preds, all_labels = 0.0, [], []\n",
    "    for prem, hyp, labels in dataloader:\n",
    "        prem, hyp, labels = prem.to(device), hyp.to(device), labels.to(device)\n",
    "        logits, _ = model(prem, hyp)\n",
    "        loss = criterion(logits, labels)\n",
    "        total_loss += loss.item()\n",
    "        all_preds.extend(logits.argmax(dim=1).cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "    return total_loss/len(dataloader), accuracy_score(all_labels, all_preds), all_preds, all_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "43f19942-497e-46c1-b14e-f141580781b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2A] Epoch 1: train 0.4869/0.7654 | val 0.5953/0.6787\n",
      "  â†³ saved best_model_2a.pth\n",
      "[2A] Epoch 2: train 0.3445/0.8517 | val 0.6330/0.6879\n",
      "  â†³ saved best_model_2a.pth\n",
      "[2A] Epoch 3: train 0.2539/0.8936 | val 0.6118/0.7255\n",
      "  â†³ saved best_model_2a.pth\n",
      "[2A] Epoch 4: train 0.1491/0.9415 | val 0.8373/0.7078\n",
      "[2A] Epoch 5: train 0.0706/0.9735 | val 1.0385/0.6986\n",
      "[2A] Epoch 6: train 0.0391/0.9860 | val 1.2304/0.7048\n",
      "[2A] Epoch 7: train 0.0306/0.9896 | val 1.7898/0.6925\n",
      "[2A] Epoch 8: train 0.0245/0.9922 | val 1.7954/0.7025\n",
      "[2A] Epoch 9: train 0.0193/0.9938 | val 1.7884/0.6963\n",
      "[2A] Epoch 10: train 0.0163/0.9952 | val 2.4001/0.6833\n"
     ]
    }
   ],
   "source": [
    "# ----- Model 2A -----\n",
    "model_2a = EncoderSideAttentionNLI(vocab_size, EMBEDDING_DIM, HIDDEN_DIM, NUM_CLASSES, pad_idx=0).to(device)\n",
    "opt_2a = optim.Adam(model_2a.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "best_val = 0.0\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    tr_loss, tr_acc = train_epoch(model_2a, train_loader, criterion, opt_2a, device)\n",
    "    va_loss, va_acc, _, _ = evaluate(model_2a, val_loader, criterion, device)\n",
    "    print(f\"[2A] Epoch {epoch+1}: train {tr_loss:.4f}/{tr_acc:.4f} | val {va_loss:.4f}/{va_acc:.4f}\")\n",
    "    if va_acc > best_val:\n",
    "        best_val = va_acc\n",
    "        torch.save(model_2a.state_dict(), \"best_model_2a.pth\")\n",
    "        print(\"  â†³ saved best_model_2a.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "057dd395-b1fb-4d4d-83dd-821e0fdada5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2A] Test: loss 0.6921 acc 0.6990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Harshit Gadhiya\\AppData\\Local\\Temp\\ipykernel_32860\\3855739586.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_2a.load_state_dict(torch.load(\"best_model_2a.pth\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     entails       0.65      0.52      0.58       842\n",
      "     neutral       0.72      0.81      0.77      1284\n",
      "\n",
      "    accuracy                           0.70      2126\n",
      "   macro avg       0.69      0.67      0.67      2126\n",
      "weighted avg       0.69      0.70      0.69      2126\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_2a.load_state_dict(torch.load(\"best_model_2a.pth\"))\n",
    "test_loss, test_acc, test_preds, test_true = evaluate(model_2a, test_loader, criterion, device)\n",
    "print(f\"[2A] Test: loss {test_loss:.4f} acc {test_acc:.4f}\")\n",
    "print(classification_report(test_true, test_preds, target_names=['entails','neutral']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4f1a68f5-027e-4edc-bad5-2aa13d5c9408",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "ix_to_word = {v:k for k,v in word_to_ix.items()}\n",
    "\n",
    "@torch.no_grad()\n",
    "def viz_encoder_attn(model, dataset, idx, topk=15):\n",
    "    model.eval()\n",
    "    prem_ids = torch.LongTensor([dataset.premise_idx[idx][:MAX_LENGTH_PREMISE]]).to(device)\n",
    "    hyp_ids  = torch.LongTensor([dataset.hypothesis_idx[idx][:MAX_LENGTH_HYPOTHESIS]]).to(device)\n",
    "    logits, alpha = model(prem_ids, hyp_ids)         # alpha: [1, Tp]\n",
    "    alpha = alpha.squeeze(0).cpu().numpy()\n",
    "    tokens = [ix_to_word.get(i, \"<UNK>\") for i in prem_ids.squeeze(0).cpu().numpy()]\n",
    "    # top-k table\n",
    "    order = np.argsort(alpha)[::-1][:topk]\n",
    "    print(\"Top-k premise tokens by attention:\")\n",
    "    for j in order:\n",
    "        print(f\"{tokens[j]:<15}  {alpha[j]:.4f}\")\n",
    "\n",
    "    # heatmap (horizontal)\n",
    "    plt.figure(figsize=(min(16, 0.6*len(tokens)), 2.8))\n",
    "    plt.imshow(alpha[np.newaxis, :], aspect='auto')\n",
    "    plt.yticks([]); plt.xticks(range(len(tokens)), tokens, rotation=90)\n",
    "    plt.title(\"Encoder-side attention over premise\")\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "# Example:\n",
    "# viz_encoder_attn(model_2a, test_dataset, idx=0, topk=15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3d7719be-e6a3-4933-a6bd-7b4df863df59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2B] Epoch 1: train 0.5297/0.7323 | val 0.6151/0.6656\n",
      "  â†³ saved best_model_2b.pth\n",
      "[2B] Epoch 2: train 0.3675/0.8374 | val 0.6734/0.6434\n",
      "[2B] Epoch 3: train 0.2644/0.8892 | val 0.7521/0.6741\n",
      "  â†³ saved best_model_2b.pth\n",
      "[2B] Epoch 4: train 0.1713/0.9313 | val 1.0444/0.6695\n",
      "[2B] Epoch 5: train 0.0996/0.9606 | val 1.2775/0.6840\n",
      "  â†³ saved best_model_2b.pth\n",
      "[2B] Epoch 6: train 0.0587/0.9789 | val 1.7307/0.6802\n",
      "[2B] Epoch 7: train 0.0387/0.9865 | val 1.5759/0.6902\n",
      "  â†³ saved best_model_2b.pth\n",
      "[2B] Epoch 8: train 0.0240/0.9923 | val 1.7992/0.6948\n",
      "  â†³ saved best_model_2b.pth\n",
      "[2B] Epoch 9: train 0.0205/0.9935 | val 2.0759/0.6925\n",
      "[2B] Epoch 10: train 0.0244/0.9919 | val 2.0109/0.6825\n",
      "[2B] Test: loss 1.8880 acc 0.7197\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     entails       0.69      0.54      0.60       842\n",
      "     neutral       0.73      0.84      0.78      1284\n",
      "\n",
      "    accuracy                           0.72      2126\n",
      "   macro avg       0.71      0.69      0.69      2126\n",
      "weighted avg       0.72      0.72      0.71      2126\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_2b = DecoderSideAttentionNLI(vocab_size, EMBEDDING_DIM, HIDDEN_DIM, NUM_CLASSES, pad_idx=0).to(device)\n",
    "opt_2b   = optim.Adam(model_2b.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "best_val = 0.0\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    tr_loss, tr_acc = train_epoch(model_2b, train_loader, criterion, opt_2b, device)\n",
    "    va_loss, va_acc, _, _ = evaluate(model_2b, val_loader, criterion, device)\n",
    "    print(f\"[2B] Epoch {epoch+1}: train {tr_loss:.4f}/{tr_acc:.4f} | val {va_loss:.4f}/{va_acc:.4f}\")\n",
    "    if va_acc > best_val:\n",
    "        best_val = va_acc\n",
    "        torch.save(model_2b.state_dict(), \"best_model_2b.pth\")\n",
    "        print(\"  â†³ saved best_model_2b.pth\")\n",
    "\n",
    "# test\n",
    "try:\n",
    "    state = torch.load(\"best_model_2b.pth\", weights_only=True)\n",
    "except TypeError:\n",
    "    state = torch.load(\"best_model_2b.pth\")\n",
    "model_2b.load_state_dict(state)\n",
    "\n",
    "test_loss, test_acc, test_preds, test_true = evaluate(model_2b, test_loader, criterion, device)\n",
    "print(f\"[2B] Test: loss {test_loss:.4f} acc {test_acc:.4f}\")\n",
    "print(classification_report(test_true, test_preds, target_names=['entails','neutral'], zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f94b847d-a92b-4ec8-8f2e-7e07110bb2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def cross_attn_heatmap(model, dataset, idx):\n",
    "    model.eval()\n",
    "    prem = torch.LongTensor([dataset.premise_idx[idx][:MAX_LENGTH_PREMISE]]).to(device)\n",
    "    hyp  = torch.LongTensor([dataset.hypothesis_idx[idx][:MAX_LENGTH_HYPOTHESIS]]).to(device)\n",
    "    logits, A = model(prem, hyp)   # A: [1, Th, Tp]\n",
    "    A = A.squeeze(0).cpu().numpy()\n",
    "    prem_toks = [ix_to_word.get(i, \"<UNK>\") for i in prem.squeeze(0).cpu().numpy()]\n",
    "    hyp_toks  = [ix_to_word.get(i, \"<UNK>\") for i in hyp.squeeze(0).cpu().numpy()]\n",
    "\n",
    "    plt.figure(figsize=(min(16, 0.4*len(prem_toks)), min(10, 0.4*len(hyp_toks))))\n",
    "    plt.imshow(A, aspect='auto')\n",
    "    plt.yticks(range(len(hyp_toks)), hyp_toks)\n",
    "    plt.xticks(range(len(prem_toks)), prem_toks, rotation=90)\n",
    "    plt.title(\"Decoder-side cross-attention (rows=hyp, cols=prem)\")\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "# Example:\n",
    "# cross_attn_heatmap(model_2b, test_dataset, idx=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3569926-a6ff-46e6-8215-f6f4f13b3c99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86de8a85-1161-45d7-902a-9f7b83f1d2fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc54dd5-0fcc-480e-963e-38322a5adb99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd1fd65-e7d4-4bcc-9c8e-c6aee5eff24a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
