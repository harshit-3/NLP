{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 2025 CITS4012 Project\n",
        "*Group No 26*"
      ],
      "metadata": {
        "id": "32yCsRUo8H33"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Readme\n",
        "*If there is something to be noted for the marker, please mention here.*\n",
        "\n",
        "*If you are planning to implement a program with Object Oriented Programming style, please put those the bottom of this ipynb file*"
      ],
      "metadata": {
        "id": "XCybYoGz8YWQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.Dataset Processing\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ],
      "metadata": {
        "id": "6po98qVA8bJD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  paths, installs, imports\n",
        "!pip -q install nltk\n",
        "\n",
        "import os, re, json, pickle\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import math\n",
        "import time\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "WBvH_EH37LIN"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "USE_DRIVE = True                  # set to False if you want to upload files manually\n",
        "DATA_DIR = \"/content/drive/MyDrive/CITS4012_Group_26\"\n",
        "\n",
        "if USE_DRIVE:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    DATA_DIR = Path(DATA_DIR)\n",
        "    assert DATA_DIR.exists(), f\"DATA_DIR not found: {DATA_DIR}\"\n",
        "else:\n",
        "    from google.colab import files\n",
        "    print(\"Please upload train.json, test.json, validation.json ...\")\n",
        "    _ = files.upload()  # choose the three JSON files\n",
        "    DATA_DIR = Path(\"/content\")"
      ],
      "metadata": {
        "id": "KBvv-Tqa9o1u",
        "outputId": "32d3a47a-5f5e-4b26-b239-ceee5a30643f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "41S-olgb_STV"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Load data\n",
        "train_path = DATA_DIR / \"train.json\"\n",
        "test_path  = DATA_DIR / \"test.json\"\n",
        "val_path   = DATA_DIR / \"validation.json\"\n",
        "\n",
        "for p in [train_path, test_path, val_path]:\n",
        "    if not p.exists():\n",
        "        raise FileNotFoundError(f\"Missing file: {p}\")\n",
        "\n",
        "train = pd.read_json(train_path)\n",
        "test  = pd.read_json(test_path)\n",
        "validation = pd.read_json(val_path)\n",
        "\n",
        "print(\"Loaded:\", len(train), len(validation), len(test))\n",
        "print(train.head(3))"
      ],
      "metadata": {
        "id": "F9O3okkc_Yao",
        "outputId": "d18a726f-72f2-4da1-dd85-58475cd7dd3f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded: 23088 1304 2126\n",
            "                                             premise  \\\n",
            "0  Pluto rotates once on its axis every 6.39 Eart...   \n",
            "1  ---Glenn =====================================...   \n",
            "2  geysers - periodic gush of hot water at the su...   \n",
            "\n",
            "                                          hypothesis    label  \n",
            "0   Earth rotates on its axis once times in one day.  neutral  \n",
            "1   Earth rotates on its axis once times in one day.  entails  \n",
            "2  The surface of the sun is much hotter than alm...  neutral  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  Cleaning\n",
        "# Keep common science symbols, but we'll also handle dots that are NOT inside numbers.\n",
        "_ALLOWED = r\"[^a-z0-9\\.\\-\\+%°/<>=,\\s]\"  # keep common science symbols\n",
        "\n",
        "def clean_text(text: str) -> str:\n",
        "    text = str(text).lower()\n",
        "\n",
        "    # replace dots not between digits with space (keeps 3.14 etc.)\n",
        "    text = re.sub(r'(?<!\\d)\\.(?!\\d)', ' ', text)\n",
        "\n",
        "    # drop disallowed chars to space\n",
        "    text = re.sub(_ALLOWED, ' ', text)\n",
        "    # keep only a-z, 0-9, dots, and whitespace (extra safety)\n",
        "    text = re.sub(r'[^a-z0-9.\\s]', ' ', text)\n",
        "    # normalize spaces\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "for df in (train, test, validation):\n",
        "    df[\"premise_clean\"] = df[\"premise\"].apply(clean_text)\n",
        "    df[\"hypothesis_clean\"] = df[\"hypothesis\"].apply(clean_text)\n",
        "\n",
        "print(\"Cleaned example:\\n\", train[[\"premise_clean\",\"hypothesis_clean\",\"label\"]].head(1))"
      ],
      "metadata": {
        "id": "JQXlFWkv_jii",
        "outputId": "1863b6bf-6849-4158-e292-226948e543cc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned example:\n",
            "                                        premise_clean  \\\n",
            "0  pluto rotates once on its axis every 6.39 eart...   \n",
            "\n",
            "                                  hypothesis_clean    label  \n",
            "0  earth rotates on its axis once times in one day  neutral  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenisation (NLTK)\n",
        "import nltk\n",
        "# Old vs new tokeniser packages; try both to be safe in Colab\n",
        "try:\n",
        "    nltk.download('punkt', quiet=True)\n",
        "except Exception as e:\n",
        "    print(\"punkt download issue:\", e)\n",
        "try:\n",
        "    nltk.download('punkt_tab', quiet=True)\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "kyeu9_TA_5U1"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Contractions (all lowercase to match our lowercasing)\n",
        "contraction_dict = {\n",
        "    \"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\",\n",
        "    \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",\n",
        "    \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\",\n",
        "    \"haven't\": \"have not\", \"he'd\": \"he would\", \"he'll\": \"he will\", \"he's\": \"he is\",\n",
        "    \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
        "    \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\", \"i'll've\": \"i will have\",\n",
        "    \"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
        "    \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\", \"it's\": \"it is\",\n",
        "    \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\n",
        "    \"mightn't\": \"might not\", \"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
        "    \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\",\n",
        "    \"needn't've\": \"need not have\", \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\",\n",
        "    \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\",\n",
        "    \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\",\n",
        "    \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\",\n",
        "    \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\n",
        "    \"so's\": \"so as\", \"this's\": \"this is\", \"that'd\": \"that would\", \"that'd've\": \"that would have\",\n",
        "    \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\",\n",
        "    \"there's\": \"there is\", \"here's\": \"here is\", \"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
        "    \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\",\n",
        "    \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\",\n",
        "    \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\",\n",
        "    \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
        "    \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\",\n",
        "    \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\",\n",
        "    \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\",\n",
        "    \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
        "    \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\",\n",
        "    \"y'all\": \"you all\", \"y'all'd\": \"you all would\", \"y'all'd've\": \"you all would have\",\n",
        "    \"y'all're\": \"you all are\", \"y'all've\": \"you all have\", \"you'd\": \"you would\",\n",
        "    \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
        "    \"you're\": \"you are\", \"you've\": \"you have\"\n",
        "}\n",
        "\n",
        "# Faster contraction replacement with regex\n",
        "_contr_pat = re.compile(r'\\b(' + '|'.join(map(re.escape, contraction_dict.keys())) + r')\\b')\n",
        "\n",
        "def _expand_contractions(s: str) -> str:\n",
        "    return _contr_pat.sub(lambda m: contraction_dict[m.group(0)], s)\n",
        "\n",
        "def pre_process(sent_list):\n",
        "    \"\"\"\n",
        "    Preprocess a list of sentences:\n",
        "      - Expand contractions\n",
        "      - Remove punctuation (after expansion)\n",
        "      - Tokenize with NLTK\n",
        "    \"\"\"\n",
        "    out = []\n",
        "    for sent in sent_list:\n",
        "        s = str(sent).lower()\n",
        "        s = _expand_contractions(s)\n",
        "        s = re.sub(r'[^\\w\\s]', '', s)  # remove punctuation\n",
        "        out.append(word_tokenize(s))\n",
        "    return out\n",
        "\n",
        "print(\"Tokenizing premises...\")\n",
        "train_premise_tokens = pre_process(train[\"premise_clean\"].tolist())\n",
        "test_premise_tokens  = pre_process(test[\"premise_clean\"].tolist())\n",
        "val_premise_tokens   = pre_process(validation[\"premise_clean\"].tolist())\n",
        "\n",
        "print(\"Tokenizing hypotheses...\")\n",
        "train_hypothesis_tokens = pre_process(train[\"hypothesis_clean\"].tolist())\n",
        "test_hypothesis_tokens  = pre_process(test[\"hypothesis_clean\"].tolist())\n",
        "val_hypothesis_tokens   = pre_process(validation[\"hypothesis_clean\"].tolist())\n",
        "\n",
        "print(\"Example tokenized premise:\", train_premise_tokens[0][:20])\n",
        "print(\"Example tokenized hypothesis:\", train_hypothesis_tokens[0][:20])"
      ],
      "metadata": {
        "id": "82lCHyj6APFs",
        "outputId": "8d0c8466-cb34-40a9-edef-a8e01a9f6f04",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizing premises...\n",
            "Tokenizing hypotheses...\n",
            "Example tokenized premise: ['pluto', 'rotates', 'once', 'on', 'its', 'axis', 'every', '639', 'earth', 'days']\n",
            "Example tokenized hypothesis: ['earth', 'rotates', 'on', 'its', 'axis', 'once', 'times', 'in', 'one', 'day']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#clip very long sequences (recommended)\n",
        "CLIP_LENGTHS = True\n",
        "PREMISE_MAX_LEN_CAP = 512    # adjust if you want\n",
        "HYP_MAX_LEN_CAP = 128             # adjust if you want\n",
        "\n",
        "# Build vocab (train only)\n",
        "PAD_TOKEN = \"<pad>\"\n",
        "UNK_TOKEN = \"<unk>\"\n",
        "word_to_ix = {PAD_TOKEN: 0, UNK_TOKEN: 1}\n",
        "\n",
        "for sent in (train_premise_tokens + train_hypothesis_tokens):\n",
        "    for w in sent:\n",
        "        if w not in word_to_ix:\n",
        "            word_to_ix[w] = len(word_to_ix)\n",
        "\n",
        "PAD_ID = word_to_ix[PAD_TOKEN]   # 0\n",
        "UNK_ID = word_to_ix[UNK_TOKEN]   # 1\n",
        "\n",
        "def to_index(token_lists, to_ix):\n",
        "    unk = to_ix[UNK_TOKEN]\n",
        "    return [[to_ix.get(w, unk) for w in sent] for sent in token_lists]\n",
        "\n",
        "vocab_size = len(word_to_ix)\n",
        "print(\"Vocab size:\", vocab_size)\n",
        "\n",
        "# Now convert all tokens → indices\n",
        "train_premise_idx = to_index(train_premise_tokens, word_to_ix)\n",
        "train_hypothesis_idx = to_index(train_hypothesis_tokens, word_to_ix)\n",
        "test_premise_idx = to_index(test_premise_tokens, word_to_ix)\n",
        "test_hypothesis_idx = to_index(test_hypothesis_tokens, word_to_ix)\n",
        "val_premise_idx = to_index(val_premise_tokens, word_to_ix)\n",
        "val_hypothesis_idx = to_index(val_hypothesis_tokens, word_to_ix)\n",
        "\n",
        "\n",
        "def _clip_lists(seqs, cap):\n",
        "    return [seq[:cap] if len(seq) > cap else seq for seq in seqs]\n",
        "\n",
        "if CLIP_LENGTHS:\n",
        "    train_premise_idx    = _clip_lists(train_premise_idx, PREMISE_MAX_LEN_CAP)\n",
        "    test_premise_idx     = _clip_lists(test_premise_idx, PREMISE_MAX_LEN_CAP)\n",
        "    val_premise_idx      = _clip_lists(val_premise_idx, PREMISE_MAX_LEN_CAP)\n",
        "    train_hypothesis_idx = _clip_lists(train_hypothesis_idx, HYP_MAX_LEN_CAP)\n",
        "    test_hypothesis_idx  = _clip_lists(test_hypothesis_idx, HYP_MAX_LEN_CAP)\n",
        "    val_hypothesis_idx   = _clip_lists(val_hypothesis_idx, HYP_MAX_LEN_CAP)\n",
        "\n",
        "# ==== Sequence length stats ===================================================\n",
        "MAX_LENGTH_PREMISE   = max(len(s) for s in train_premise_idx) if train_premise_idx else 0\n",
        "MAX_LENGTH_HYPOTHESIS = max(len(s) for s in train_hypothesis_idx) if train_hypothesis_idx else 0\n",
        "\n",
        "print(f\"Max premise length (train): {MAX_LENGTH_PREMISE}\")\n",
        "print(f\"Max hypothesis length (train): {MAX_LENGTH_HYPOTHESIS}\")"
      ],
      "metadata": {
        "id": "AIQb31xLAgMI",
        "outputId": "89ec14fa-5490-4d15-c41e-b9031e5ee220",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab size: 20499\n",
            "Max premise length (train): 512\n",
            "Max hypothesis length (train): 36\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Labels\n",
        "label_to_ix = {\"entails\": 0, \"neutral\": 1}\n",
        "ix_to_label = {v:k for k,v in label_to_ix.items()}\n",
        "\n",
        "train_labels = [label_to_ix[l] for l in train[\"label\"]]\n",
        "test_labels  = [label_to_ix[l] for l in test[\"label\"]]\n",
        "val_labels   = [label_to_ix[l] for l in validation[\"label\"]]"
      ],
      "metadata": {
        "id": "hDB8VTN6A85C"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Final stats\n",
        "print(f\"Training examples:   {len(train_premise_idx)}\")\n",
        "print(f\"Validation examples: {len(val_premise_idx)}\")\n",
        "print(f\"Test examples:       {len(test_premise_idx)}\")\n",
        "print(f\"Vocabulary size:     {vocab_size}\")\n",
        "print(f\"Num labels:          {len(label_to_ix)}\")"
      ],
      "metadata": {
        "id": "xnNeNBgHBZr5",
        "outputId": "960aa61a-6514-422a-ee22-3eaba898fa81",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training examples:   23088\n",
            "Validation examples: 1304\n",
            "Test examples:       2126\n",
            "Vocabulary size:     20499\n",
            "Num labels:          2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sanity printout\n",
        "print(\"Variables are ready for model training!\")\n",
        "print(f\"Vocab size:           {vocab_size}\")\n",
        "print(f\"Max Premise Length:   {MAX_LENGTH_PREMISE}\")\n",
        "print(f\"Max Hypothesis Length:{MAX_LENGTH_HYPOTHESIS}\")\n",
        "print(f\"Label mapping:        {label_to_ix}\")\n",
        "print(f\"Example premise idx:  {train_premise_idx[0][:15]}\")\n"
      ],
      "metadata": {
        "id": "CzMMazqOC4sN",
        "outputId": "ae0f756d-4886-41b5-82dd-24cc2fc7a6ca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Variables are ready for model training!\n",
            "Vocab size:           20499\n",
            "Max Premise Length:   512\n",
            "Max Hypothesis Length:36\n",
            "Label mapping:        {'entails': 0, 'neutral': 1}\n",
            "Example premise idx:  [2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check sequence length statistics\n",
        "premise_lens = [len(p) for p in train_premise_idx]\n",
        "hyp_lens = [len(h) for h in train_hypothesis_idx]\n",
        "\n",
        "print(f\"Premise lengths - Max: {max(premise_lens)}, Mean: {np.mean(premise_lens):.1f}, 95th percentile: {np.percentile(premise_lens, 95):.1f}\")\n",
        "print(f\"Hypothesis lengths - Max: {max(hyp_lens)}, Mean: {np.mean(hyp_lens):.1f}, 95th percentile: {np.percentile(hyp_lens, 95):.1f}\")\n",
        "#print(f\"\\nNote: Sequences will be truncated to {200} tokens to manage memory\")"
      ],
      "metadata": {
        "id": "YIkDNLuA7LMF",
        "outputId": "5838a43c-5ca6-4017-93ce-38c063192f6d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Premise lengths - Max: 512, Mean: 17.4, 95th percentile: 33.0\n",
            "Hypothesis lengths - Max: 36, Mean: 11.8, 95th percentile: 20.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\" Using device: {device}\")\n"
      ],
      "metadata": {
        "id": "AyPN4wQ37LRK",
        "outputId": "d084875c-8036-43c7-97e8-d05d34a16d82",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset class\n",
        "class NLIDataset(Dataset):\n",
        "    def __init__(self, premise_idx, hypothesis_idx, labels):\n",
        "        self.premise_idx = premise_idx\n",
        "        self.hypothesis_idx = hypothesis_idx\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            'premise': self.premise_idx[idx],\n",
        "            'hypothesis': self.hypothesis_idx[idx],\n",
        "            'label': self.labels[idx]\n",
        "        }"
      ],
      "metadata": {
        "id": "n3xKam6e7LT4"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_SEQ_LENGTH = 400\n",
        "PAD_ID = 0\n",
        "\n",
        "def collate_fn(batch):\n",
        "    premises = [item['premise'][:MAX_LENGTH_PREMISE] for item in batch]\n",
        "    hypotheses = [item['hypothesis'][:MAX_LENGTH_HYPOTHESIS] for item in batch]\n",
        "    labels = [item['label'] for item in batch]\n",
        "\n",
        "    max_premise_len = max(len(p) for p in premises)\n",
        "    max_hypothesis_len = max(len(h) for h in hypotheses)\n",
        "\n",
        "    def pad_to(seqs, L):\n",
        "        return [seq + [PAD_ID]*(L - len(seq)) for seq in seqs]\n",
        "\n",
        "    premises_tensor   = torch.LongTensor(pad_to(premises,   max_premise_len))\n",
        "    hypotheses_tensor = torch.LongTensor(pad_to(hypotheses, max_hypothesis_len))\n",
        "    labels_tensor     = torch.LongTensor(labels)\n",
        "\n",
        "    return premises_tensor, hypotheses_tensor, labels_tensor"
      ],
      "metadata": {
        "id": "iliJTe7MD4XR"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create datasets\n",
        "train_dataset = NLIDataset(train_premise_idx, train_hypothesis_idx, train_labels)\n",
        "val_dataset = NLIDataset(val_premise_idx, val_hypothesis_idx, val_labels)\n",
        "test_dataset = NLIDataset(test_premise_idx, test_hypothesis_idx, test_labels)\n",
        "\n",
        "# hyperparameters - automatically adjust batch size based on GPU\n",
        "BATCH_SIZE = 64 if torch.cuda.is_available() else 32  # use 64 for GPU, 32 for CPU\n",
        "EMBEDDING_DIM = 128\n",
        "HIDDEN_DIM = 256\n",
        "NUM_CLASSES = 2\n",
        "LEARNING_RATE = 0.001\n",
        "NUM_EPOCHS = 10\n",
        "\n",
        "print(f\"Hyperparameters:\")\n",
        "print(f\"  Batch size: {BATCH_SIZE}\")\n",
        "print(f\"  Embedding dim: {EMBEDDING_DIM}\")\n",
        "print(f\"  Hidden dim: {HIDDEN_DIM}\")\n",
        "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
        "print(f\"  Number of epochs: {NUM_EPOCHS}\")\n",
        "\n",
        "# create dataloaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "print(f\"\\nDataloaders created:\")\n",
        "print(f\"  Training batches: {len(train_loader)}\")\n",
        "print(f\"  Validation batches: {len(val_loader)}\")\n",
        "print(f\"  Test batches: {len(test_loader)}\")"
      ],
      "metadata": {
        "id": "pU4-KkfAEExg",
        "outputId": "cc3773c9-f79a-4c67-b94d-deb6e323d538",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hyperparameters:\n",
            "  Batch size: 64\n",
            "  Embedding dim: 128\n",
            "  Hidden dim: 256\n",
            "  Learning rate: 0.001\n",
            "  Number of epochs: 10\n",
            "\n",
            "Dataloaders created:\n",
            "  Training batches: 361\n",
            "  Validation batches: 21\n",
            "  Test batches: 34\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Model Implementation\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ],
      "metadata": {
        "id": "1FA2ao2l8hOg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reproducibility\n",
        "import random\n",
        "def set_seed(s=1337):\n",
        "    random.seed(s); np.random.seed(s); torch.manual_seed(s)\n",
        "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(s)\n",
        "set_seed(1337)\n"
      ],
      "metadata": {
        "id": "7cZXT2O-SrzX"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model 1: Siamese BiLSTM"
      ],
      "metadata": {
        "id": "2QHrabakENqY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# siamese bilstm model\n",
        "class SiameseBiLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_classes):\n",
        "        super(SiameseBiLSTM, self).__init__()\n",
        "\n",
        "        # embedding layer\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "\n",
        "        # bidirectional lstm\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
        "\n",
        "        # classification layers\n",
        "        self.fc1 = nn.Linear(4 * hidden_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "\n",
        "    def encode(self, x):\n",
        "        # embed sequence\n",
        "        embeds = self.embedding(x)\n",
        "\n",
        "        # pass through bilstm\n",
        "        lstm_out, (h_n, c_n) = self.lstm(embeds)\n",
        "\n",
        "        # concat forward and backward final hidden states\n",
        "        hidden = torch.cat((h_n[0], h_n[1]), dim=1)\n",
        "        return hidden\n",
        "\n",
        "    def forward(self, premise, hypothesis):\n",
        "        # encode both sequences with shared encoder\n",
        "        premise_vec = self.encode(premise)\n",
        "        hypothesis_vec = self.encode(hypothesis)\n",
        "\n",
        "        # concatenate encodings\n",
        "        combined = torch.cat((premise_vec, hypothesis_vec), dim=1)\n",
        "\n",
        "        # classification\n",
        "        out = torch.relu(self.fc1(combined))\n",
        "        out = self.dropout(out)\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "yf8xqk-Qj8Cf"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize model\n",
        "model = SiameseBiLSTM(vocab_size, EMBEDDING_DIM, HIDDEN_DIM, NUM_CLASSES).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "print(\"Model initialized\")\n",
        "print(f\"Total parameters: {sum(p.numel() for p in model.parameters())}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hgVwwCbZkcvu",
        "outputId": "29d07c40-2eca-4f0a-d2fa-dfb31d435849"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model initialized\n",
            "Total parameters: 3677314\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# training function\n",
        "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    for premises, hypotheses, labels in dataloader:\n",
        "        premises = premises.to(device)\n",
        "        hypotheses = hypotheses.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(premises, hypotheses)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # get predictions\n",
        "        preds = torch.argmax(outputs, dim=1)\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    return avg_loss, accuracy"
      ],
      "metadata": {
        "id": "WmXNX2qXkc8n"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluation function\n",
        "def evaluate(model, dataloader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for premises, hypotheses, labels in dataloader:\n",
        "            premises = premises.to(device)\n",
        "            hypotheses = hypotheses.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(premises, hypotheses)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    return avg_loss, accuracy, all_preds, all_labels"
      ],
      "metadata": {
        "id": "rXYMaCdhkwRH"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training loop\n",
        "best_val_acc = 0\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "train_accs = []\n",
        "val_accs = []\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "    val_loss, val_acc, _, _ = evaluate(model, val_loader, criterion, device)\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "    train_accs.append(train_acc)\n",
        "    val_accs.append(val_acc)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS}\")\n",
        "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
        "    print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    # save best model\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        torch.save(model.state_dict(), 'best_siamese_bilstm.pth')\n",
        "        print(f\"Best model saved with validation accuracy: {best_val_acc:.4f}\")\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qaEQaAeYkwWX",
        "outputId": "e97c724b-5598-4018-9fbe-6da4a7b578e1"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "Train Loss: 0.5032, Train Acc: 0.7526\n",
            "Val Loss: 0.5784, Val Acc: 0.6656\n",
            "--------------------------------------------------\n",
            "Best model saved with validation accuracy: 0.6656\n",
            "\n",
            "Epoch 2/10\n",
            "Train Loss: 0.3519, Train Acc: 0.8468\n",
            "Val Loss: 0.5943, Val Acc: 0.6840\n",
            "--------------------------------------------------\n",
            "Best model saved with validation accuracy: 0.6840\n",
            "\n",
            "Epoch 3/10\n",
            "Train Loss: 0.2594, Train Acc: 0.8903\n",
            "Val Loss: 0.6061, Val Acc: 0.7048\n",
            "--------------------------------------------------\n",
            "Best model saved with validation accuracy: 0.7048\n",
            "\n",
            "Epoch 4/10\n",
            "Train Loss: 0.1719, Train Acc: 0.9288\n",
            "Val Loss: 0.7197, Val Acc: 0.7078\n",
            "--------------------------------------------------\n",
            "Best model saved with validation accuracy: 0.7078\n",
            "\n",
            "Epoch 5/10\n",
            "Train Loss: 0.0958, Train Acc: 0.9628\n",
            "Val Loss: 1.0207, Val Acc: 0.7017\n",
            "--------------------------------------------------\n",
            "\n",
            "Epoch 6/10\n",
            "Train Loss: 0.0543, Train Acc: 0.9806\n",
            "Val Loss: 1.2284, Val Acc: 0.7009\n",
            "--------------------------------------------------\n",
            "\n",
            "Epoch 7/10\n",
            "Train Loss: 0.0328, Train Acc: 0.9887\n",
            "Val Loss: 1.6158, Val Acc: 0.6817\n",
            "--------------------------------------------------\n",
            "\n",
            "Epoch 8/10\n",
            "Train Loss: 0.0239, Train Acc: 0.9923\n",
            "Val Loss: 1.6572, Val Acc: 0.6948\n",
            "--------------------------------------------------\n",
            "\n",
            "Epoch 9/10\n",
            "Train Loss: 0.0219, Train Acc: 0.9925\n",
            "Val Loss: 1.5754, Val Acc: 0.6917\n",
            "--------------------------------------------------\n",
            "\n",
            "Epoch 10/10\n",
            "Train Loss: 0.0182, Train Acc: 0.9946\n",
            "Val Loss: 1.7329, Val Acc: 0.7025\n",
            "--------------------------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# plot training history\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "# plot loss\n",
        "ax1.plot(train_losses, label='Train Loss')\n",
        "ax1.plot(val_losses, label='Val Loss')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Loss')\n",
        "ax1.set_title('Training and Validation Loss')\n",
        "ax1.legend()\n",
        "\n",
        "# plot accuracy\n",
        "ax2.plot(train_accs, label='Train Acc')\n",
        "ax2.plot(val_accs, label='Val Acc')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Accuracy')\n",
        "ax2.set_title('Training and Validation Accuracy')\n",
        "ax2.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "id": "A7LrSqihlGxs",
        "outputId": "1a46f75a-f1be-41a4-d224-d07252dd08da"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x400 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wkkyEd0SlG1V"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vGjIo3kClG4V"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model 2: Encoder-Decoder\n",
        "*In both variants, the premise and hypothesis sentences are first tokenized, embedded, and processed. Model 2A applies self-attention on the encoder side, using an additive attention layer to highlight salient words within the premise before combining it with the hypothesis representation. Model 2B instead adopts decoder-side (Bahdanau) cross-attention, where each decoding step for the hypothesis dynamically attends to the encoded premise states, creating explicit word-to-word alignment between the two sentences. Both models share the same preprocessing pipeline, embedding dimension, hidden size, and training loop as the baseline BiLSTM, ensuring a fair comparison. We evaluate both variants on the same NLI dataset and include attention-weight visualizations to qualitatively analyse how each model focuses on relevant premise tokens during inference.*"
      ],
      "metadata": {
        "id": "OeZn4lscEv86"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Common training/eval utilities (reuse across models)\n",
        "criterion = nn.CrossEntropyLoss()  # or with label_smoothing=0.05\n",
        "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
        "    model.train(); tot=0; preds=[]; gold=[]\n",
        "    for prem, hyp, y in dataloader:\n",
        "        prem, hyp, y = prem.to(device), hyp.to(device), y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        logits, _ = model(prem, hyp)\n",
        "        loss = criterion(logits, y)\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        tot += loss.item()\n",
        "        preds.extend(logits.argmax(1).detach().cpu().numpy())\n",
        "        gold.extend(y.cpu().numpy())\n",
        "    return tot/len(dataloader), accuracy_score(gold, preds)\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, dataloader, criterion, device):\n",
        "    model.eval(); tot=0; preds=[]; gold=[]\n",
        "    for prem, hyp, y in dataloader:\n",
        "        prem, hyp, y = prem.to(device), hyp.to(device), y.to(device)\n",
        "        logits, _ = model(prem, hyp)\n",
        "        loss = criterion(logits, y)\n",
        "        tot += loss.item()\n",
        "        preds.extend(logits.argmax(1).cpu().numpy())\n",
        "        gold.extend(y.cpu().numpy())\n",
        "    return tot/len(dataloader), accuracy_score(gold, preds), preds, gold\n"
      ],
      "metadata": {
        "id": "0ctorphNUIOc"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Model 2A — Encoder-side attention (self-attn over premise)"
      ],
      "metadata": {
        "id": "Q4gpXmeHTPR6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class SelfAttentionPool(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.W = nn.Linear(dim, dim)\n",
        "        self.v = nn.Linear(dim, 1, bias=False)\n",
        "    def forward(self, H, mask):\n",
        "        scores = self.v(torch.tanh(self.W(H))).squeeze(-1)\n",
        "        scores = scores.masked_fill(mask == 0, -1e9)\n",
        "        all_pad = (mask.sum(1) == 0)\n",
        "        if all_pad.any():\n",
        "            scores[all_pad] = -1e9; scores[all_pad,0] = 0.0\n",
        "        alpha = torch.softmax(scores, dim=-1)\n",
        "        p_star = torch.bmm(alpha.unsqueeze(1), H).squeeze(1)\n",
        "        return p_star, alpha\n",
        "\n",
        "class EncoderSideAttentionNLI(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_dim, hid_dim, num_classes, pad_idx=0, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=pad_idx)\n",
        "        self.enc_p = nn.LSTM(emb_dim, hid_dim, batch_first=True, bidirectional=True)\n",
        "        self.enc_h = nn.LSTM(emb_dim, hid_dim, batch_first=True, bidirectional=True)\n",
        "        self.pool_p = SelfAttentionPool(2*hid_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc1 = nn.Linear(8*hid_dim, 2*hid_dim)\n",
        "        self.fc2 = nn.Linear(2*hid_dim, num_classes)\n",
        "    def _mask(self, x): return (x != 0).long()\n",
        "    def _encode(self, x, lstm):\n",
        "        H, _ = lstm(self.embedding(x))\n",
        "        return H\n",
        "    def _max_pool(self, H, mask):\n",
        "        masked = H.masked_fill(mask.unsqueeze(-1)==0, float('-inf'))\n",
        "        pooled = masked.max(1).values\n",
        "        is_all_pad = (mask.sum(1) == 0)\n",
        "        if is_all_pad.any(): pooled[is_all_pad] = 0.0\n",
        "        return pooled\n",
        "    def forward(self, prem_ids, hyp_ids):\n",
        "        prem_mask, hyp_mask = self._mask(prem_ids), self._mask(hyp_ids)\n",
        "        H_p = self._encode(prem_ids, self.enc_p)\n",
        "        p_star, alpha = self.pool_p(H_p, prem_mask)\n",
        "        H_h = self._encode(hyp_ids, self.enc_h)\n",
        "        h_star = self._max_pool(H_h, hyp_mask)\n",
        "        z = torch.cat([p_star, h_star, torch.abs(p_star-h_star), p_star*h_star], dim=-1)\n",
        "        out = self.dropout(torch.relu(self.fc1(z)))\n",
        "        logits = self.fc2(out)\n",
        "        return logits, alpha\n"
      ],
      "metadata": {
        "id": "7Z7cSpx3I8kz"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_2a = EncoderSideAttentionNLI(vocab_size, EMBEDDING_DIM, HIDDEN_DIM, NUM_CLASSES, pad_idx=0).to(device)\n",
        "opt_2a = optim.Adam(model_2a.parameters(), lr=LEARNING_RATE)\n",
        "best = 0.0\n",
        "for e in range(NUM_EPOCHS):\n",
        "    tr_l,tr_a = train_epoch(model_2a, train_loader, criterion, opt_2a, device)\n",
        "    va_l,va_a,_,_ = evaluate(model_2a, val_loader, criterion, device)\n",
        "    print(f\"[2A] {e+1}: train {tr_l:.4f}/{tr_a:.4f} | val {va_l:.4f}/{va_a:.4f}\")\n",
        "    if va_a>best: best=va_a; torch.save(model_2a.state_dict(),\"best_model_2a.pth\")\n",
        "# test\n",
        "model_2a.load_state_dict(torch.load(\"best_model_2a.pth\"))\n",
        "tl, ta, tp, tt = evaluate(model_2a, test_loader, criterion, device)\n",
        "print(f\"[2A] Test: loss {tl:.4f} acc {ta:.4f}\")\n",
        "print(classification_report(tt, tp, target_names=['entails','neutral'], zero_division=0))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0klqGi6JT7zp",
        "outputId": "9876bdd4-3dda-4002-b1e7-2978bb65d859"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2A] 1: train 0.4918/0.7614 | val 0.6281/0.6794\n",
            "[2A] 2: train 0.3489/0.8506 | val 0.5911/0.6917\n",
            "[2A] 3: train 0.2522/0.8957 | val 0.6248/0.7040\n",
            "[2A] 4: train 0.1475/0.9439 | val 0.8744/0.6994\n",
            "[2A] 5: train 0.0733/0.9739 | val 1.1064/0.6933\n",
            "[2A] 6: train 0.0389/0.9869 | val 1.2855/0.6994\n",
            "[2A] 7: train 0.0277/0.9912 | val 1.3430/0.6956\n",
            "[2A] 8: train 0.0227/0.9927 | val 1.6391/0.6833\n",
            "[2A] 9: train 0.0181/0.9944 | val 1.6750/0.7132\n",
            "[2A] 10: train 0.0176/0.9951 | val 1.7142/0.6940\n",
            "[2A] Test: loss 1.8506 acc 0.6990\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     entails       0.65      0.53      0.58       842\n",
            "     neutral       0.72      0.81      0.76      1284\n",
            "\n",
            "    accuracy                           0.70      2126\n",
            "   macro avg       0.69      0.67      0.67      2126\n",
            "weighted avg       0.69      0.70      0.69      2126\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Model 2B — Decoder-side (Bahdanau cross-attn)"
      ],
      "metadata": {
        "id": "j68BW_owTb8K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BahdanauAttention(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.W_h = nn.Linear(dim, dim, bias=False)\n",
        "        self.W_s = nn.Linear(dim, dim, bias=False)\n",
        "        self.v   = nn.Linear(dim, 1,  bias=False)\n",
        "\n",
        "    def forward(self, Hk, q, mask):\n",
        "        scores = self.v(torch.tanh(self.W_h(Hk) + self.W_s(q).unsqueeze(1))).squeeze(-1)\n",
        "        scores = scores.masked_fill(mask==0, -1e9)\n",
        "        all_pad = (mask.sum(1)==0)\n",
        "        if all_pad.any():\n",
        "            scores[all_pad] = -1e9; scores[all_pad,0] = 0.0\n",
        "        alpha = torch.softmax(scores, dim=-1)\n",
        "        ctx = torch.bmm(alpha.unsqueeze(1), Hk).squeeze(1)\n",
        "        return ctx, alpha\n",
        "\n",
        "class DecoderSideAttentionNLI(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_dim, hid_dim, num_classes, pad_idx=0, dropout=0.4):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=pad_idx)\n",
        "        self.enc_p = nn.LSTM(emb_dim, hid_dim, batch_first=True, bidirectional=True)\n",
        "        self.dec_h = nn.LSTM(emb_dim + 2*hid_dim, hid_dim, batch_first=True)\n",
        "        self.attn  = BahdanauAttention(2*hid_dim)\n",
        "        self.proj_q = nn.Linear(hid_dim, 2*hid_dim, bias=False)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc1 = nn.Linear(hid_dim + 2*hid_dim, 2*hid_dim)\n",
        "        self.fc2 = nn.Linear(2*hid_dim, num_classes)\n",
        "\n",
        "    def forward(self, prem_ids, hyp_ids):\n",
        "        B, Th = hyp_ids.size(0), hyp_ids.size(1)\n",
        "        prem_mask = (prem_ids != 0).long()\n",
        "        H_p, _ = self.enc_p(self.embedding(prem_ids))\n",
        "        emb_h = self.embedding(hyp_ids)\n",
        "        h_t = torch.zeros(1,B,self.dec_h.hidden_size, device=emb_h.device)\n",
        "        c_t = torch.zeros(1,B,self.dec_h.hidden_size, device=emb_h.device)\n",
        "        ctx_prev = torch.zeros(B, H_p.size(-1), device=emb_h.device)\n",
        "        outs, attn_all = [], []\n",
        "        for t in range(Th):\n",
        "            dec_in = torch.cat([emb_h[:,t,:], ctx_prev], dim=-1).unsqueeze(1)\n",
        "            dec_out, (h_t,c_t) = self.dec_h(dec_in, (h_t,c_t))\n",
        "            s_t = dec_out.squeeze(1)\n",
        "            q_t = self.proj_q(s_t)\n",
        "            ctx_t, alpha_t = self.attn(H_p, q_t, prem_mask)\n",
        "            outs.append(torch.cat([s_t, ctx_t], dim=-1))\n",
        "            attn_all.append(alpha_t.unsqueeze(1))\n",
        "            ctx_prev = ctx_t\n",
        "        dec_stack = torch.stack(outs, dim=1)\n",
        "        hyp_mask = (hyp_ids != 0).long()\n",
        "        mask3 = hyp_mask.unsqueeze(-1).float()\n",
        "        g = (dec_stack*mask3).sum(1) / hyp_mask.sum(1).clamp(min=1).unsqueeze(-1).float()\n",
        "        out = self.dropout(torch.relu(self.fc1(g)))\n",
        "        logits = self.fc2(out)\n",
        "        A = torch.cat(attn_all, dim=1) if attn_all else None\n",
        "        return logits, A\n"
      ],
      "metadata": {
        "id": "54KzKQuRTavY"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_2b = DecoderSideAttentionNLI(vocab_size, EMBEDDING_DIM, HIDDEN_DIM, NUM_CLASSES, pad_idx=0).to(device)\n",
        "opt_2b = optim.Adam(model_2b.parameters(), lr=LEARNING_RATE, weight_decay=1e-5)\n",
        "best=0.0; patience, wait = 3, 0\n",
        "\n",
        "for e in range(NUM_EPOCHS):\n",
        "    tr_l,tr_a = train_epoch(model_2b, train_loader, criterion, opt_2b, device)\n",
        "    va_l,va_a,_,_ = evaluate(model_2b, val_loader, criterion, device)\n",
        "    print(f\"[2B] {e+1}: train {tr_l:.4f}/{tr_a:.4f} | val {va_l:.4f}/{va_a:.4f}\")\n",
        "    if va_a>best: best, wait = va_a, 0; torch.save(model_2b.state_dict(),\"best_model_2b.pth\")\n",
        "\n",
        "model_2b.load_state_dict(torch.load(\"best_model_2b.pth\"))\n",
        "tl, ta, tp, tt = evaluate(model_2b, test_loader, criterion, device)\n",
        "print(f\"[2B] Test: loss {tl:.4f} acc {ta:.4f}\")\n",
        "print(classification_report(tt, tp, target_names=['entails','neutral'], zero_division=0))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rNfCYOm1UxSo",
        "outputId": "5378b18d-9444-4cba-ae16-bc597dba5c7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2B] 1: train 0.5148/0.7428 | val 0.5613/0.6979\n",
            "[2B] 2: train 0.3656/0.8407 | val 0.5258/0.7224\n",
            "[2B] 3: train 0.2826/0.8804 | val 0.6629/0.6994\n",
            "[2B] 4: train 0.2051/0.9163 | val 0.8712/0.6879\n",
            "[2B] 5: train 0.1387/0.9439 | val 0.9687/0.7109\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kg7213_4sA0C"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model 3: Transformer"
      ],
      "metadata": {
        "id": "DLZmQkGeEwGq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PADDING_IDX = 0\n",
        "CLS_IDX = vocab_size          # new id after existing vocab\n",
        "SEP_IDX = vocab_size + 1      # new id after existing vocab\n",
        "VOCAB_SIZE = vocab_size + 2   # total size including CLS/SEP\n",
        "\n",
        "# ----- Build concatenated sequences: [CLS] premise [SEP] hypothesis [SEP] -----\n",
        "MAX_SEQ_LEN = 256\n",
        "\n",
        "def concatenate_and_pad(p_idx_list, h_idx_list, max_len, padding_idx, cls_idx, sep_idx):\n",
        "    combined_sequences = []\n",
        "    for p_idx, h_idx in zip(p_idx_list, h_idx_list):\n",
        "        combined = [cls_idx] + p_idx + [sep_idx] + h_idx + [sep_idx]\n",
        "        if len(combined) > max_len:\n",
        "            combined = combined[:max_len]\n",
        "        elif len(combined) < max_len:\n",
        "            combined = combined + [padding_idx] * (max_len - len(combined))\n",
        "        combined_sequences.append(combined)\n",
        "    return combined_sequences\n",
        "\n",
        "print(f\"Concatenation & padding sequences to {MAX_SEQ_LEN}...\")\n",
        "train_sequence_idx = concatenate_and_pad(train_premise_idx, train_hypothesis_idx, MAX_SEQ_LEN, PADDING_IDX, CLS_IDX, SEP_IDX)\n",
        "val_sequence_idx   = concatenate_and_pad(val_premise_idx,   val_hypothesis_idx,   MAX_SEQ_LEN, PADDING_IDX, CLS_IDX, SEP_IDX)\n",
        "test_sequence_idx  = concatenate_and_pad(test_premise_idx,  test_hypothesis_idx,  MAX_SEQ_LEN, PADDING_IDX, CLS_IDX, SEP_IDX)\n",
        "print(\"Concatenation & padding complete.\")\n"
      ],
      "metadata": {
        "id": "I3Vh-He1ci9u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19a3a76d-9eb8-43b5-aa2a-cd35e9a03ee2"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Concatenation & padding sequences to 256...\n",
            "Concatenation & padding complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe.unsqueeze(0))  # [1, max_len, d_model]\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:     # x: [B,T,E]\n",
        "        x = x + self.pe[:, :x.size(1), :]\n",
        "        return self.dropout(x)\n",
        "\n",
        "def create_padding_mask(input_indices, padding_idx):\n",
        "    return (input_indices == padding_idx)\n",
        "\n",
        "# ----- Dataset -----\n",
        "class NliDataset(Dataset):\n",
        "    def __init__(self, sequence_idx, labels):\n",
        "        self.sequence_idx = torch.tensor(np.array(sequence_idx), dtype=torch.long)\n",
        "        self.labels = torch.tensor(np.array(labels), dtype=torch.long)\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.sequence_idx[idx], self.labels[idx]"
      ],
      "metadata": {
        "id": "6jA9b4lnof3U"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, max_seq_len, num_layers, num_heads,\n",
        "                 dim_feedforward, dropout, num_classes, padding_idx):\n",
        "        super().__init__()\n",
        "        self.padding_idx = padding_idx\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=padding_idx)\n",
        "        self.pos_encoder = PositionalEncoding(embedding_dim, dropout, max_len=max_seq_len)\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=embedding_dim,\n",
        "            nhead=num_heads,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "            dropout=dropout,\n",
        "            batch_first=True,          # use [B,T,E]\n",
        "            activation='gelu'\n",
        "        )\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers)\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(embedding_dim, dim_feedforward),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(dim_feedforward, num_classes)\n",
        "        )\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        # Only init the classifier explicitly; embeddings/encoder use defaults\n",
        "        for name, param in self.named_parameters():\n",
        "            if 'classifier' in name and 'weight' in name:\n",
        "                nn.init.uniform_(param, -initrange, initrange)\n",
        "            elif 'classifier' in name and 'bias' in name:\n",
        "                nn.init.zeros_(param)\n",
        "\n",
        "    def forward(self, combined_ix):                         # combined_ix: [B,T]\n",
        "        key_padding_mask = create_padding_mask(combined_ix, self.padding_idx)  # [B,T] True at PAD\n",
        "        x = self.embedding(combined_ix)                    # [B,T,E]\n",
        "        x = self.pos_encoder(x)                            # [B,T,E]\n",
        "        enc = self.transformer_encoder(x, src_key_padding_mask=key_padding_mask)  # [B,T,E]\n",
        "        cls_vec = enc[:, 0, :]                             # take [CLS] position\n",
        "        logits = self.classifier(cls_vec)                  # [B,2]\n",
        "        return logits"
      ],
      "metadata": {
        "id": "uVdZWMOnokSB"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EMBEDDING_DIM    = 256\n",
        "NUM_LAYERS       = 3\n",
        "NUM_HEADS        = 4\n",
        "DIM_FEEDFORWARD  = 4 * EMBEDDING_DIM\n",
        "DROPOUT          = 0.1\n",
        "NUM_CLASSES      = 2\n",
        "LEARNING_RATE    = 1e-4\n",
        "BATCH_SIZE       = 32\n",
        "NUM_EPOCHS       = 10\n",
        "MODEL_SAVE_PATH  = \"best_transformer.pth\"\n",
        "\n",
        "# ----- Model, criterion, optim -----\n",
        "transformer_model = Transformer(\n",
        "    vocab_size=VOCAB_SIZE,\n",
        "    embedding_dim=EMBEDDING_DIM,\n",
        "    max_seq_len=MAX_SEQ_LEN,\n",
        "    num_layers=NUM_LAYERS,\n",
        "    num_heads=NUM_HEADS,\n",
        "    dim_feedforward=DIM_FEEDFORWARD,\n",
        "    dropout=DROPOUT,\n",
        "    num_classes=NUM_CLASSES,\n",
        "    padding_idx=PADDING_IDX\n",
        ").to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(transformer_model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5)\n",
        "\n",
        "# ----- DataLoaders -----\n",
        "train_dataset = NliDataset(train_sequence_idx, train_labels)\n",
        "val_dataset   = NliDataset(val_sequence_idx,   val_labels)\n",
        "test_dataset  = NliDataset(test_sequence_idx,  test_labels)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader   = DataLoader(val_dataset,   batch_size=BATCH_SIZE, shuffle=False)\n",
        "test_loader  = DataLoader(test_dataset,  batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# ----- Train / Eval loops -----\n",
        "def train_one_epoch(model, dataloader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    for seqs, labels in dataloader:\n",
        "        seqs, labels = seqs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(seqs)\n",
        "        loss = criterion(logits, labels)\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / max(1, len(dataloader))\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_model(model, dataloader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    all_preds, all_labels = [], []\n",
        "    for seqs, labels in dataloader:\n",
        "        seqs, labels = seqs.to(device), labels.to(device)\n",
        "        logits = model(seqs)\n",
        "        loss = criterion(logits, labels)\n",
        "        total_loss += loss.item()\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "    return total_loss / max(1, len(dataloader)), acc, all_preds, all_labels\n",
        "\n",
        "# ----- Training -----\n",
        "best_val_acc = 0.0\n",
        "print(\"\\nStarting Training:\")\n",
        "for epoch in range(1, NUM_EPOCHS + 1):\n",
        "    train_loss = train_one_epoch(transformer_model, train_loader, criterion, optimizer, device)\n",
        "    val_loss, val_acc, _, _ = evaluate_model(transformer_model, val_loader, criterion, device)\n",
        "    print(f\"[M3] Epoch {epoch}/{NUM_EPOCHS} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        torch.save(transformer_model.state_dict(), MODEL_SAVE_PATH)\n",
        "        print(f\"  ↳ Saved best model with Val Acc: {best_val_acc:.4f}\")\n",
        "\n",
        "# ----- Final Test Eval -----\n",
        "print(\"\\nReport:\")\n",
        "state = torch.load(MODEL_SAVE_PATH)\n",
        "transformer_model.load_state_dict(state)\n",
        "transformer_model.to(device)\n",
        "\n",
        "test_loss, test_acc, test_preds, test_true = evaluate_model(transformer_model, test_loader, criterion, device)\n",
        "print(f\"[M3] Test Loss: {test_loss:.4f}\")\n",
        "print(f\"[M3] Test Accuracy: {test_acc:.4f}\")\n",
        "print(classification_report(test_true, test_preds, target_names=['entails', 'neutral'], zero_division=0))"
      ],
      "metadata": {
        "id": "GYN0mLh1cmB2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e9e0e28-2e1d-46a6-f715-59ece72df7ba"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting Training:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:515: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. We recommend specifying layout=torch.jagged when constructing a nested tensor, as this layout receives active development, has better operator coverage, and works with torch.compile. (Triggered internally at /pytorch/aten/src/ATen/NestedTensorImpl.cpp:178.)\n",
            "  output = torch._nested_tensor_from_mask(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[M3] Epoch 1/10 | Train Loss: 0.6533 | Val Loss: 0.6172 | Val Acc: 0.6549\n",
            "  ↳ Saved best model with Val Acc: 0.6549\n",
            "[M3] Epoch 2/10 | Train Loss: 0.5475 | Val Loss: 0.7033 | Val Acc: 0.6549\n",
            "[M3] Epoch 3/10 | Train Loss: 0.4857 | Val Loss: 0.6522 | Val Acc: 0.6564\n",
            "  ↳ Saved best model with Val Acc: 0.6564\n",
            "[M3] Epoch 4/10 | Train Loss: 0.4388 | Val Loss: 0.6546 | Val Acc: 0.6710\n",
            "  ↳ Saved best model with Val Acc: 0.6710\n",
            "[M3] Epoch 5/10 | Train Loss: 0.4078 | Val Loss: 0.8247 | Val Acc: 0.6304\n",
            "[M3] Epoch 6/10 | Train Loss: 0.3814 | Val Loss: 0.6207 | Val Acc: 0.6702\n",
            "[M3] Epoch 7/10 | Train Loss: 0.3600 | Val Loss: 0.6767 | Val Acc: 0.6741\n",
            "  ↳ Saved best model with Val Acc: 0.6741\n",
            "[M3] Epoch 8/10 | Train Loss: 0.3368 | Val Loss: 0.6947 | Val Acc: 0.6710\n",
            "[M3] Epoch 9/10 | Train Loss: 0.3188 | Val Loss: 0.6675 | Val Acc: 0.6610\n",
            "[M3] Epoch 10/10 | Train Loss: 0.2953 | Val Loss: 0.9860 | Val Acc: 0.6518\n",
            "\n",
            "Report:\n",
            "[M3] Test Loss: 0.6714\n",
            "[M3] Test Accuracy: 0.6938\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     entails       0.69      0.41      0.52       842\n",
            "     neutral       0.70      0.88      0.78      1284\n",
            "\n",
            "    accuracy                           0.69      2126\n",
            "   macro avg       0.69      0.65      0.65      2126\n",
            "weighted avg       0.69      0.69      0.67      2126\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.Testing and Evaluation\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ],
      "metadata": {
        "id": "EzGuzHPE87Ya"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================\n",
        "# Unified Testing & Evaluation\n",
        "# ============================\n",
        "import os\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import pandas as pd\n",
        "\n",
        "def safe_load_state(path):\n",
        "    \"\"\"Load state_dict with weights_only=True when available (newer PyTorch),\n",
        "    fallback to default for older versions.\"\"\"\n",
        "    try:\n",
        "        return torch.load(path, weights_only=True)\n",
        "    except TypeError:\n",
        "        return torch.load(path)\n",
        "\n",
        "def evaluate_and_report(model, weight_path, model_name):\n",
        "    if model is None:\n",
        "        print(f\"[{model_name}] SKIPPED (model not defined).\")\n",
        "        return None\n",
        "    if not os.path.exists(weight_path):\n",
        "        print(f\"[{model_name}] SKIPPED (weights not found at: {weight_path}).\")\n",
        "        return None\n",
        "\n",
        "    print(f\"\\n[{model_name}] Loading weights from: {weight_path}\")\n",
        "    state = safe_load_state(weight_path)\n",
        "    model.load_state_dict(state)\n",
        "    tl, ta, preds, gold = evaluate(model, test_loader, criterion, device)\n",
        "    print(f\"[{model_name}] Test: loss {tl:.4f} acc {ta:.4f}\")\n",
        "    print(classification_report(gold, preds, target_names=['entails','neutral'], zero_division=0))\n",
        "\n",
        "    # Confusion matrix (uncomment if you want the plots)\n",
        "    # cm = confusion_matrix(gold, preds)\n",
        "    # ConfusionMatrixDisplay(cm, display_labels=['entails','neutral']).plot(cmap='Blues')\n",
        "    # plt.title(f'{model_name} — Test Confusion Matrix'); plt.show()\n",
        "\n",
        "    return {\"Model\": model_name, \"Test Loss\": tl, \"Test Acc\": ta, \"Preds\": preds, \"Gold\": gold}\n",
        "\n",
        "results = []\n",
        "\n",
        "# -----------------\n",
        "# Model 1: Siamese BiLSTM (commented until your teammate plugs it in)\n",
        "# -----------------\n",
        "# model_1 = SiameseBiLSTM(vocab_size, EMBEDDING_DIM, HIDDEN_DIM, NUM_CLASSES).to(device)\n",
        "# results.append(evaluate_and_report(model_1, \"best_siamese_bilstm.pth\", \"Model 1 — BiLSTM\"))\n",
        "\n",
        "# -----------------\n",
        "# Model 2A: Encoder-side Attention (yours)\n",
        "# -----------------\n",
        "try:\n",
        "    results.append(evaluate_and_report(model_2a, \"best_model_2a.pth\", \"Model 2A — Enc Self-Attn\"))\n",
        "except NameError:\n",
        "    print(\"[Model 2A] SKIPPED (model_2a not in scope).\")\n",
        "\n",
        "# -----------------\n",
        "# Model 2B: Decoder-side Cross-Attention (yours)\n",
        "# -----------------\n",
        "try:\n",
        "    results.append(evaluate_and_report(model_2b, \"best_model_2b.pth\", \"Model 2B — Dec Cross-Attn\"))\n",
        "except NameError:\n",
        "    print(\"[Model 2B] SKIPPED (model_2b not in scope).\")\n",
        "\n",
        "# -----------------\n",
        "# Model 3: Transformer (commented until your teammate plugs it in)\n",
        "# -----------------\n",
        "# model_3 = YourTransformerClass(...).to(device)\n",
        "# results.append(evaluate_and_report(model_3, \"best_transformer.pth\", \"Model 3 — Transformer\"))\n",
        "\n",
        "# ---- Summary table ----\n",
        "results = [r for r in results if r is not None]\n",
        "if results:\n",
        "    summary_rows = [{\"Model\": r[\"Model\"], \"Test Loss\": round(r[\"Test Loss\"], 4), \"Test Acc\": round(r[\"Test Acc\"], 4)} for r in results]\n",
        "    df_summary = pd.DataFrame(summary_rows).sort_values(by=\"Test Acc\", ascending=False)\n",
        "    print(\"\\n=== Test Summary ===\")\n",
        "    display(df_summary)\n",
        "else:\n",
        "    print(\"\\nNo models evaluated — define models or provide weight files first.\")\n",
        "\n",
        "# (Optional) save summary + raw preds to disk\n",
        "try:\n",
        "    if results:\n",
        "        df_summary.to_csv(\"test_summary.csv\", index=False)\n",
        "        # Save per-model detailed preds\n",
        "        for r in results:\n",
        "            pd.DataFrame({\"gold\": r[\"Gold\"], \"pred\": r[\"Preds\"]}).to_csv(\n",
        "                f\"{r['Model'].replace(' ','_')}_test_preds.csv\", index=False\n",
        "            )\n",
        "        print(\"Saved: test_summary.csv and *_test_preds.csv\")\n",
        "except Exception as e:\n",
        "    print(\"Save error:\", e)\n"
      ],
      "metadata": {
        "id": "6ZVeNYIH9IaL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a1dbe39-c04a-483a-cb4c-00842b17ed52"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Model 2A] SKIPPED (model_2a not in scope).\n",
            "[Model 2B] SKIPPED (model_2b not in scope).\n",
            "\n",
            "No models evaluated — define models or provide weight files first.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Object Oriented Programming codes here\n",
        "\n",
        "*You can use multiple code snippets. Just add more if needed*"
      ],
      "metadata": {
        "id": "mefSOe8eTmGP"
      }
    }
  ]
}